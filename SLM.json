[
    {
        "Model": "facebook/opt-125m",
        "Parameters": "125M",
        "Type": "",
        "ReleaseDate": "2022.05",
        "affiliation": "Facebook     ",
        "Link": "https://huggingface.co/facebook/opt-125m",
        "AttentionType": "MHA",
        "LayerNumber": "12",
        "HiddenSize": "768",
        "HeadNum": "12",
        "Activation": "relu",
        "VocabularySize": "50272",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "180B Token Dataset",
        "TrainingDatasets": "RoBERTa, The Pile, PushShift.io Reddit",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.40330891430100174",
        "Problemsolving": "0.2939773037630773",
        "Math": "1.8999999999999997",
        "IncontextLearning": ""
    },
    {
        "Model": "facebook/opt-350m",
        "Parameters": "350M",
        "Type": "",
        "ReleaseDate": "2022.05",
        "affiliation": "Facebook     ",
        "Link": "https://huggingface.co/facebook/opt-350m",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "1024",
        "HeadNum": "16",
        "Activation": "relu",
        "VocabularySize": "50272",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "180B Token Dataset",
        "TrainingDatasets": "RoBERTa, The Pile, PushShift.io Reddit",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.41397459998536906",
        "Problemsolving": "0.30746496119248734",
        "Math": "1.5333333333333332",
        "IncontextLearning": ""
    },
    {
        "Model": "facebook/opt-2.7b",
        "Parameters": "2.7B",
        "Type": "",
        "ReleaseDate": "2022.05",
        "affiliation": "Facebook     ",
        "Link": "https://huggingface.co/facebook/opt-2.7b",
        "AttentionType": "MHA",
        "LayerNumber": "32",
        "HiddenSize": "2560",
        "HeadNum": "32",
        "Activation": "relu",
        "VocabularySize": "50272",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "180B Token Dataset",
        "TrainingDatasets": "RoBERTa, The Pile, PushShift.io Reddit",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4718290255748334",
        "Problemsolving": "0.4062225475593449",
        "Math": "2.433333333333333",
        "IncontextLearning": ""
    },
    {
        "Model": "facebook/opt-1.3b",
        "Parameters": "1.3B",
        "Type": "",
        "ReleaseDate": "2022.05",
        "affiliation": "Facebook     ",
        "Link": "https://huggingface.co/facebook/opt-1.3b",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "32",
        "Activation": "relu",
        "VocabularySize": "50272",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "180B Token Dataset",
        "TrainingDatasets": "RoBERTa, The Pile, PushShift.io Reddit",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4489786977209106",
        "Problemsolving": "0.3765542415709661",
        "Math": "2.2",
        "IncontextLearning": ""
    },
    {
        "Model": "facebook/galactica-125m",
        "Parameters": "125M",
        "Type": "",
        "ReleaseDate": "2022.11",
        "affiliation": "Facebook     ",
        "Link": "https://huggingface.co/facebook/galactica-125m",
        "AttentionType": "MHA",
        "LayerNumber": "12",
        "HiddenSize": "768",
        "HeadNum": "12",
        "Activation": "gelu",
        "VocabularySize": "50000",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "450B/106B Dataset",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.3966410279781424",
        "Problemsolving": "0.311034022",
        "Math": "1.9333333333333336",
        "IncontextLearning": ""
    },
    {
        "Model": "facebook/galactica-1.3b",
        "Parameters": "1.3B",
        "Type": "",
        "ReleaseDate": "2022.11",
        "affiliation": "Facebook     ",
        "Link": "https://huggingface.co/facebook/galactica-1.3b",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "32",
        "Activation": "gelu",
        "VocabularySize": "50000",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "450B/106B Dataset",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.42972719135376486",
        "Problemsolving": "0.4020213226480942",
        "Math": "2.8666666666666667",
        "IncontextLearning": ""
    },
    {
        "Model": "bigscience/bloom-560m",
        "Parameters": "560M",
        "Type": "",
        "ReleaseDate": "2022.11",
        "affiliation": "BigScience   ",
        "Link": "https://huggingface.co/bigscience/bloom-560m",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "1024",
        "HeadNum": "16",
        "Activation": "gelu_new ",
        "VocabularySize": "250880",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "350B Dataset",
        "TrainingDatasets": "WuDaoCorpora",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.40277299396556165",
        "Problemsolving": "0.32298484",
        "Math": "1.6333333333333335",
        "IncontextLearning": ""
    },
    {
        "Model": "bigscience/bloom-1b1",
        "Parameters": "1.1B",
        "Type": "",
        "ReleaseDate": "2022.11",
        "affiliation": "BigScience   ",
        "Link": "https://huggingface.co/bigscience/bloom-1b1",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "1536",
        "HeadNum": "16",
        "Activation": "gelu_new ",
        "VocabularySize": "250880",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "350B Dataset",
        "TrainingDatasets": "WuDaoCorpora",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.42811859839462024",
        "Problemsolving": "0.3593767378915064",
        "Math": "2.4",
        "IncontextLearning": ""
    },
    {
        "Model": "bigscience/bloomz-1b1",
        "Parameters": "1.1B",
        "Type": "",
        "ReleaseDate": "2022.11",
        "affiliation": "BigScience   ",
        "Link": "https://huggingface.co/bigscience/bloomz-1b1",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "1536",
        "HeadNum": "16",
        "Activation": "gelu_new ",
        "VocabularySize": "250880",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "350B Dataset & 0.5B Finetune",
        "TrainingDatasets": "WuDaoCorpora",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.434554381",
        "Problemsolving": "0.3344127132507441",
        "Math": "2.033333333333333",
        "IncontextLearning": ""
    },
    {
        "Model": "bigscience/bloomz-560m",
        "Parameters": "560M",
        "Type": "",
        "ReleaseDate": "2022.11",
        "affiliation": "BigScience   ",
        "Link": "https://huggingface.co/bigscience/bloomz-560m",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "1024",
        "HeadNum": "16",
        "Activation": "gelu_new ",
        "VocabularySize": "250880",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "350B Dataset & 0.36B Finetune",
        "TrainingDatasets": "WuDaoCorpora",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4241245157639775",
        "Problemsolving": "0.3086482218874575",
        "Math": "1.2333333333333334",
        "IncontextLearning": ""
    },
    {
        "Model": "EleutherAI/pythia-410m",
        "Parameters": "410M",
        "Type": "",
        "ReleaseDate": "2023.03",
        "affiliation": "EleutherAI   ",
        "Link": "https://huggingface.co/EleutherAI/pythia-410m",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "1024",
        "HeadNum": "16",
        "Activation": "gelu",
        "VocabularySize": "50304",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "300B/207B Dataset",
        "TrainingDatasets": "The Pile",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.42236839186775155",
        "Problemsolving": "0.35428525247152415",
        "Math": "1.8333333333333333",
        "IncontextLearning": ""
    },
    {
        "Model": "EleutherAI/pythia-160m",
        "Parameters": "160M",
        "Type": "",
        "ReleaseDate": "2023.03",
        "affiliation": "EleutherAI   ",
        "Link": "https://huggingface.co/EleutherAI/pythia-160m",
        "AttentionType": "MHA",
        "LayerNumber": "12",
        "HiddenSize": "768",
        "HeadNum": "12",
        "Activation": "gelu",
        "VocabularySize": "50304",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "300B/207B Dataset",
        "TrainingDatasets": "The Pile",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.3971187304404128",
        "Problemsolving": "0.29676108803870566",
        "Math": "2.3666666666666667",
        "IncontextLearning": ""
    },
    {
        "Model": "EleutherAI/pythia-1b",
        "Parameters": "1B",
        "Type": "",
        "ReleaseDate": "2023.03",
        "affiliation": "EleutherAI   ",
        "Link": "https://huggingface.co/EleutherAI/pythia-1b",
        "AttentionType": "MHA",
        "LayerNumber": "16",
        "HiddenSize": "2048",
        "HeadNum": "8",
        "Activation": "gelu",
        "VocabularySize": "50304",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "300B/207B Dataset",
        "TrainingDatasets": "The Pile",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4391897616795198",
        "Problemsolving": "0.3718107301998688",
        "Math": "2.4666666666666663",
        "IncontextLearning": ""
    },
    {
        "Model": "EleutherAI/pythia-1.4b",
        "Parameters": "1.4B",
        "Type": "",
        "ReleaseDate": "2023.03",
        "affiliation": "EleutherAI   ",
        "Link": "https://huggingface.co/EleutherAI/pythia-1.4b",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "16",
        "Activation": "gelu",
        "VocabularySize": "50304",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "300B/207B Dataset",
        "TrainingDatasets": "The Pile",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4515981152524496",
        "Problemsolving": "0.3950810544217184",
        "Math": "2.4",
        "IncontextLearning": ""
    },
    {
        "Model": "EleutherAI/pythia-2.8b",
        "Parameters": "2.8B",
        "Type": "",
        "ReleaseDate": "2023.03",
        "affiliation": "EleutherAI   ",
        "Link": "https://huggingface.co/EleutherAI/pythia-2.8b",
        "AttentionType": "MHA",
        "LayerNumber": "32",
        "HiddenSize": "2560",
        "HeadNum": "32",
        "Activation": "gelu",
        "VocabularySize": "50304",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "300B/207B Dataset",
        "TrainingDatasets": "The Pile",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4745200104093684",
        "Problemsolving": "0.42091012861480187",
        "Math": "2.4666666666666663",
        "IncontextLearning": ""
    },
    {
        "Model": "cerebras/Cerebras-GPT-111M",
        "Parameters": "111M",
        "Type": "",
        "ReleaseDate": "2023.03",
        "affiliation": "Cerebras     ",
        "Link": "https://huggingface.co/cerebras/Cerebras-GPT-111M",
        "AttentionType": "MHA",
        "LayerNumber": "10",
        "HiddenSize": "768",
        "HeadNum": "12",
        "Activation": "gelu",
        "VocabularySize": "50257",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "2.2B/371B Dataset",
        "TrainingDatasets": "the Pile",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "Maximal Update Parameterization (\u00b5P)",
        "Commonsensereasoning": "0.3985193686023088",
        "Problemsolving": "0.26197499636213645",
        "Math": "1.3666666666666665",
        "IncontextLearning": ""
    },
    {
        "Model": "cerebras/Cerebras-GPT-256M",
        "Parameters": "256M",
        "Type": "",
        "ReleaseDate": "2023.03",
        "affiliation": "Cerebras     ",
        "Link": "https://huggingface.co/cerebras/Cerebras-GPT-256M",
        "AttentionType": "MHA",
        "LayerNumber": "14",
        "HiddenSize": "1088",
        "HeadNum": "17",
        "Activation": "gelu",
        "VocabularySize": "50257",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "5.1B/371B Dataset",
        "TrainingDatasets": "the Pile",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "Maximal Update Parameterization (\u00b5P)",
        "Commonsensereasoning": "0.40752859976291816",
        "Problemsolving": "0.2906337730091724",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "cerebras/Cerebras-GPT-590M",
        "Parameters": "590M",
        "Type": "",
        "ReleaseDate": "2023.03",
        "affiliation": "Cerebras     ",
        "Link": "https://huggingface.co/cerebras/Cerebras-GPT-590M",
        "AttentionType": "MHA",
        "LayerNumber": "18",
        "HiddenSize": "1536",
        "HeadNum": "12",
        "Activation": "gelu",
        "VocabularySize": "50257",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "11.8B/371B Dataset",
        "TrainingDatasets": "the Pile",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "Maximal Update Parameterization (\u00b5P)",
        "Commonsensereasoning": "0.40610391965762194",
        "Problemsolving": "0.3091130972394746",
        "Math": "1.8333333333333333",
        "IncontextLearning": ""
    },
    {
        "Model": "cerebras/Cerebras-GPT-1.3B",
        "Parameters": "1.3B",
        "Type": "",
        "ReleaseDate": "2023.03",
        "affiliation": "Cerebras     ",
        "Link": "https://huggingface.co/cerebras/Cerebras-GPT-1.3B",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "16",
        "Activation": "gelu",
        "VocabularySize": "50257",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "26.3B/371B Dataset",
        "TrainingDatasets": "the Pile",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "Maximal Update Parameterization (\u00b5P)",
        "Commonsensereasoning": "0.4195949441210137",
        "Problemsolving": "0.3460961752544626",
        "Math": "1.8333333333333333",
        "IncontextLearning": ""
    },
    {
        "Model": "cerebras/Cerebras-GPT-2.7B",
        "Parameters": "2.7B",
        "Type": "",
        "ReleaseDate": "2023.03",
        "affiliation": "Cerebras     ",
        "Link": "https://huggingface.co/cerebras/Cerebras-GPT-2.7B",
        "AttentionType": "MHA",
        "LayerNumber": "32",
        "HiddenSize": "2560",
        "HeadNum": "32",
        "Activation": "gelu",
        "VocabularySize": "50257",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "53.0B/371B Dataset",
        "TrainingDatasets": "the Pile",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "Maximal Update Parameterization (\u00b5P)",
        "Commonsensereasoning": "0.43581390624597566",
        "Problemsolving": "0.3751688275751184",
        "Math": "2.566666666666667",
        "IncontextLearning": ""
    },
    {
        "Model": "MBZUAI/LaMini-GPT-774M",
        "Parameters": "774M",
        "Type": "",
        "ReleaseDate": "2023.04",
        "affiliation": "MBZUAI       ",
        "Link": "https://huggingface.co/MBZUAI/LaMini-GPT-774M",
        "AttentionType": "MHA",
        "LayerNumber": "36",
        "HiddenSize": "1280",
        "HeadNum": "20",
        "Activation": "gelu_new ",
        "VocabularySize": "50258",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "1024",
        "TrainingInnovations": "Distillation",
        "Commonsensereasoning": "0.4613248111509908",
        "Problemsolving": "0.3776879817137882",
        "Math": "1.8",
        "IncontextLearning": ""
    },
    {
        "Model": "MBZUAI/LaMini-GPT-1.5B",
        "Parameters": "1.5B",
        "Type": "",
        "ReleaseDate": "2023.04",
        "affiliation": "MBZUAI       ",
        "Link": "https://huggingface.co/MBZUAI/LaMini-GPT-1.5B",
        "AttentionType": "MHA",
        "LayerNumber": "48",
        "HiddenSize": "1600",
        "HeadNum": "25",
        "Activation": "gelu_new ",
        "VocabularySize": "50258",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "1024",
        "TrainingInnovations": "Distillation",
        "Commonsensereasoning": "0.4889008138689614",
        "Problemsolving": "0.417691761",
        "Math": "1.7666666666666666",
        "IncontextLearning": ""
    },
    {
        "Model": "microsoft/phi-1_5",
        "Parameters": "1.3B",
        "Type": "",
        "ReleaseDate": "2023.09",
        "affiliation": "Microsoft  ",
        "Link": "https://huggingface.co/microsoft/phi-1_5",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "32",
        "Activation": "gelu_new",
        "VocabularySize": "51200",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "150B/26B Dataset",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "32xA100-40G*8days",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5826252800231556",
        "Problemsolving": "0.5720886943097728",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "microsoft/phi-1",
        "Parameters": "1.3B",
        "Type": "",
        "ReleaseDate": "2023.09",
        "affiliation": "Microsoft  ",
        "Link": "https://huggingface.co/microsoft/phi-1",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "32",
        "Activation": "gelu_new",
        "VocabularySize": "51200",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "54B/7B Dataset",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "8 A100*6days",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.3681939631119856",
        "Problemsolving": "0.27931778300580884",
        "Math": "2.266666666666667",
        "IncontextLearning": ""
    },
    {
        "Model": "stabilityai/stablelm-zephyr-3b",
        "Parameters": "3B",
        "Type": "instruct",
        "ReleaseDate": "2023.11",
        "affiliation": "StabilityAI",
        "Link": "https://huggingface.co/stabilityai/stablelm-zephyr-3b",
        "AttentionType": "MHA",
        "LayerNumber": "32",
        "HiddenSize": "2560",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "50304",
        "FFNratio": "2.7",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "400B/100B Dataset",
        "TrainingDatasets": "Falcon RefinedWeb extract, RedPajama-Data, The Pile, StarCoder",
        "GPUhours": "64 A100-80G",
        "MaxContextWindow": "1024",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.6292983635093926",
        "Problemsolving": "0.5338538262737792",
        "Math": "29.63333333333333",
        "IncontextLearning": ""
    },
    {
        "Model": "Qwen/Qwen-1_8B",
        "Parameters": "1.8B",
        "Type": "",
        "ReleaseDate": "2023.11",
        "affiliation": "Alibaba",
        "Link": "https://huggingface.co/Qwen/Qwen-1_8B",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "16",
        "Activation": "silu",
        "VocabularySize": "151936",
        "FFNratio": "5",
        "ArchitectureInnovation": "dynamic NTK-aware interpolation; LogN-Scaling; window attention",
        "TrainingTokens": "2200B/2200B Dataset",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "8192",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5413025360048237",
        "Problemsolving": "0.4910581139459918",
        "Math": "18.45",
        "IncontextLearning": ""
    },
    {
        "Model": "microsoft/phi-2",
        "Parameters": "2.7B",
        "Type": "",
        "ReleaseDate": "2023.12",
        "affiliation": "Microsoft  ",
        "Link": "https://huggingface.co/microsoft/phi-2",
        "AttentionType": "MHA",
        "LayerNumber": "32",
        "HiddenSize": "2560",
        "HeadNum": "32",
        "Activation": "gelu_new ",
        "VocabularySize": "51200",
        "FFNratio": "4",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "1400B/250B Dataset",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "96xA100-80G*14Days",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.6396376339295174",
        "Problemsolving": "0.6658064580833533",
        "Math": "26.833333333333332",
        "IncontextLearning": ""
    },
    {
        "Model": "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T",
        "Parameters": "1.1B",
        "Type": "",
        "ReleaseDate": "2023.12",
        "affiliation": "TinyLlama    ",
        "Link": "https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T",
        "AttentionType": "GQA",
        "LayerNumber": "22",
        "HiddenSize": "2048",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "2.75",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "3000B",
        "TrainingDatasets": "SlimPajama, StarCoder Training Dataset",
        "GPUhours": "16*A100-40G*90d",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.46278690564437774",
        "Problemsolving": "0.40886390798301825",
        "Math": "2.6",
        "IncontextLearning": ""
    },
    {
        "Model": "mtgv/MobileLLaMA-1.4B-Base",
        "Parameters": "1.4B",
        "Type": "",
        "ReleaseDate": "2023.12",
        "affiliation": "Meituan",
        "Link": "https://huggingface.co/mtgv/MobileLLaMA-1.4B-Base",
        "AttentionType": "GQA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "16",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "2.75",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "1300B",
        "TrainingDatasets": "RedPajama v1",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4519731920859205",
        "Problemsolving": "0.4047563334022784",
        "Math": "2.1333333333333333",
        "IncontextLearning": ""
    },
    {
        "Model": "stabilityai/stablelm-2-zephyr-1_6b",
        "Parameters": "1.6B",
        "Type": "instruct",
        "ReleaseDate": "2024.01",
        "affiliation": "StabilityAI  ",
        "Link": "https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "100352",
        "FFNratio": "2.75",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "2000B",
        "TrainingDatasets": "Falcon RefinedWeb extract, RedPajama-Data, The Pile, StarCoder, CulturaX",
        "GPUhours": "92k",
        "MaxContextWindow": "4096",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5970204844773325",
        "Problemsolving": "0.5151239490774924",
        "Math": "23.100000000000005",
        "IncontextLearning": ""
    },
    {
        "Model": "Qwen/Qwen1.5-0.5B",
        "Parameters": "0.5B",
        "Type": "",
        "ReleaseDate": "2024.02",
        "affiliation": "Alibaba",
        "Link": "https://huggingface.co/Qwen/Qwen1.5-0.5B",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "1024",
        "HeadNum": "16",
        "Activation": "silu",
        "VocabularySize": "151936",
        "FFNratio": "3",
        "ArchitectureInnovation": "mixture of sliding window attention and full attention",
        "TrainingTokens": "2400B",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "32000",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.46914097356943324",
        "Problemsolving": "0.42949667281308307",
        "Math": "7.333333333333333",
        "IncontextLearning": ""
    },
    {
        "Model": "Qwen/Qwen1.5-1.8B",
        "Parameters": "1.8B",
        "Type": "",
        "ReleaseDate": "2024.02",
        "affiliation": "Alibaba",
        "Link": "https://huggingface.co/Qwen/Qwen1.5-1.8B",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "16",
        "Activation": "silu",
        "VocabularySize": "151936",
        "FFNratio": "3",
        "ArchitectureInnovation": "mixture of sliding window attention and full attention",
        "TrainingTokens": "2400B",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "32000",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5565065178944251",
        "Problemsolving": "0.4981422786718504",
        "Math": "18.633333333333333",
        "IncontextLearning": ""
    },
    {
        "Model": "Qwen/Qwen1.5-4B",
        "Parameters": "4B",
        "Type": "",
        "ReleaseDate": "2024.02",
        "affiliation": "Alibaba",
        "Link": "https://huggingface.co/Qwen/Qwen1.5-4B",
        "AttentionType": "MHA",
        "LayerNumber": "40",
        "HiddenSize": "2560",
        "HeadNum": "20",
        "Activation": "silu",
        "VocabularySize": "151936",
        "FFNratio": "3",
        "ArchitectureInnovation": "mixture of sliding window attention and full attention",
        "TrainingTokens": "4000B",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "32000",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.6217040770819489",
        "Problemsolving": "0.5904057492043817",
        "Math": "21.766666666666666",
        "IncontextLearning": ""
    },
    {
        "Model": "MBZUAI/MobiLlama-1B",
        "Parameters": "1B",
        "Type": "",
        "ReleaseDate": "2024.02",
        "affiliation": "MBZUAI       ",
        "Link": "https://huggingface.co/MBZUAI/MobiLlama-1B",
        "AttentionType": "GQA",
        "LayerNumber": "22",
        "HiddenSize": "2048",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "2.75",
        "ArchitectureInnovation": "shared FFN",
        "TrainingTokens": "1259B",
        "TrainingDatasets": "RedPajama-v1, Amber dataset uses RefinedWeb to replace common_crawl subset of RedPajama-v1",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4676336545025897",
        "Problemsolving": "0.40131655250916354",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "MBZUAI/MobiLlama-05B",
        "Parameters": "0.5B",
        "Type": "",
        "ReleaseDate": "2024.02",
        "affiliation": "MBZUAI       ",
        "Link": "https://huggingface.co/MBZUAI/MobiLlama-05B",
        "AttentionType": "GQA",
        "LayerNumber": "22",
        "HiddenSize": "2048",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "2.75",
        "ArchitectureInnovation": "shared FFN",
        "TrainingTokens": "1259B",
        "TrainingDatasets": "RedPajama-v1, Amber dataset uses RefinedWeb to replace common_crawl subset of RedPajama-v2",
        "GPUhours": "26..6k(7d)",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.44128476773462205",
        "Problemsolving": "0.3679364962584417",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "google/gemma-2b",
        "Parameters": "2B",
        "Type": "",
        "ReleaseDate": "2024.02",
        "affiliation": "Google ",
        "Link": "https://huggingface.co/google/gemma-2b",
        "AttentionType": "MQA",
        "LayerNumber": "18",
        "HiddenSize": "2048",
        "HeadNum": "8",
        "Activation": "gelu",
        "VocabularySize": "256000",
        "FFNratio": "8",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "3000B",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "512 TPU-V5e",
        "MaxContextWindow": "8192",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5453290860104331",
        "Problemsolving": "0.5498431176054166",
        "Math": "13.299999999999999",
        "IncontextLearning": ""
    },
    {
        "Model": "h2oai/h2o-danube2-1.8b-base",
        "Parameters": "1.8B",
        "Type": "",
        "ReleaseDate": "2024.03",
        "affiliation": "H2O",
        "Link": "https://huggingface.co/h2oai/h2o-danube2-1.8b-base",
        "AttentionType": "GQA",
        "LayerNumber": "24",
        "HiddenSize": "2560",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "1T",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5686333882295481",
        "Problemsolving": "0.4048900956807783",
        "Math": "21.45",
        "IncontextLearning": ""
    },
    {
        "Model": "openbmb/MiniCPM-1B-sft-bf16",
        "Parameters": "1B",
        "Type": "Instruct",
        "ReleaseDate": "2024.04",
        "affiliation": "OpenBMB      ",
        "Link": "https://huggingface.co/openbmb/MiniCPM-1B-sft-bf16",
        "AttentionType": "GQA",
        "LayerNumber": "52",
        "HiddenSize": "1536",
        "HeadNum": "24",
        "Activation": "silu",
        "VocabularySize": "73440",
        "FFNratio": "2.5",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "128k",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5786006652448227",
        "Problemsolving": "0.5434731807245381",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "apple/OpenELM-270M",
        "Parameters": "270M",
        "Type": "",
        "ReleaseDate": "2024.04",
        "affiliation": "Apple        ",
        "Link": "https://huggingface.co/apple/OpenELM-270M",
        "AttentionType": "GQA",
        "LayerNumber": "16",
        "HiddenSize": "1280",
        "HeadNum": "12-20",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "0",
        "ArchitectureInnovation": "Layer-wise scaling",
        "TrainingTokens": "1500B",
        "TrainingDatasets": "RefinedWeb, deduplicated PILE, a subset of RedPajama, and a subset of Dolma v1.6",
        "GPUhours": "128*A100*3d",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4330385542360417",
        "Problemsolving": "0.3527958832951137",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "apple/OpenELM-450M",
        "Parameters": "450M",
        "Type": "",
        "ReleaseDate": "2024.04",
        "affiliation": "Apple        ",
        "Link": "https://huggingface.co/apple/OpenELM-450M",
        "AttentionType": "GQA",
        "LayerNumber": "20",
        "HiddenSize": "1536",
        "HeadNum": "12-24",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "0",
        "ArchitectureInnovation": "Layer-wise scaling",
        "TrainingTokens": "1500B",
        "TrainingDatasets": "RefinedWeb, deduplicated PILE, a subset of RedPajama, and a subset of Dolma v1.7",
        "GPUhours": "128*H100*3",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4577673492456106",
        "Problemsolving": "0.3762858648915779",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "apple/OpenELM-1_1B",
        "Parameters": "1.1B",
        "Type": "",
        "ReleaseDate": "2024.04",
        "affiliation": "Apple        ",
        "Link": "https://huggingface.co/apple/OpenELM-1_1B",
        "AttentionType": "GQA",
        "LayerNumber": "28",
        "HiddenSize": "2048",
        "HeadNum": "16-32",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "0",
        "ArchitectureInnovation": "Layer-wise scaling",
        "TrainingTokens": "1500B",
        "TrainingDatasets": "RefinedWeb, deduplicated PILE, a subset of RedPajama, and a subset of Dolma v1.8",
        "GPUhours": "128*A100*11",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.49943928024068784",
        "Problemsolving": "0.42567132396350654",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "apple/OpenELM-3B",
        "Parameters": "3B",
        "Type": "",
        "ReleaseDate": "2024.04",
        "affiliation": "Apple        ",
        "Link": "https://huggingface.co/apple/OpenELM-3B",
        "AttentionType": "MHA",
        "LayerNumber": "36",
        "HiddenSize": "3072",
        "HeadNum": "12-24",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "0",
        "ArchitectureInnovation": "Layer-wise scaling",
        "TrainingTokens": "1500B",
        "TrainingDatasets": "RefinedWeb, deduplicated PILE, a subset of RedPajama, and a subset of Dolma v1.9",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "None",
        "Problemsolving": "None",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "openbmb/MiniCPM-2B-128k",
        "Parameters": "2B",
        "Type": "",
        "ReleaseDate": "2024.04",
        "affiliation": "OpenBMB  ",
        "Link": "https://huggingface.co/openbmb/MiniCPM-2B-128k",
        "AttentionType": "GQA",
        "LayerNumber": "40",
        "HiddenSize": "2304",
        "HeadNum": "36",
        "Activation": "silu",
        "VocabularySize": "122760",
        "FFNratio": "2.5",
        "ArchitectureInnovation": "none",
        "TrainingTokens": "",
        "TrainingDatasets": "cloed dataset",
        "GPUhours": "",
        "MaxContextWindow": "131072",
        "TrainingInnovations": "learning rate scheduler\u3001Two Stage Pre-training Strategy",
        "Commonsensereasoning": "0.5742625254831465",
        "Problemsolving": "0.5369604236985314",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "microsoft/Phi-3-mini-4k-instruct",
        "Parameters": "3.8B",
        "Type": "instruct",
        "ReleaseDate": "2024.04",
        "affiliation": "Microsoft  ",
        "Link": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct",
        "AttentionType": "GQA",
        "LayerNumber": "32",
        "HiddenSize": "3072",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "32064",
        "FFNratio": "2.6666666666666665",
        "ArchitectureInnovation": "blocksparse attention",
        "TrainingTokens": "3300B",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "512 H100-80G*10Days",
        "MaxContextWindow": "4096/128000",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.6755007915031034",
        "Problemsolving": "0.7242779089373833",
        "Math": "55.23333333333333",
        "IncontextLearning": ""
    },
    {
        "Model": "google/recurrentgemma-2b",
        "Parameters": "2B",
        "Type": "",
        "ReleaseDate": "2024.04",
        "affiliation": "Google",
        "Link": "https://huggingface.co/google/recurrentgemma-2b",
        "AttentionType": "MQA",
        "LayerNumber": "26",
        "HiddenSize": "2560",
        "HeadNum": "10",
        "Activation": "gelu_pytorch_tanh",
        "VocabularySize": "256000",
        "FFNratio": "6",
        "ArchitectureInnovation": "Griffin architecture",
        "TrainingTokens": "2000B",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "tpu-v5e",
        "MaxContextWindow": "8192",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5353327118359907",
        "Problemsolving": "0.5057644433307263",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "Qwen/Qwen2-0.5B",
        "Parameters": "0.5B",
        "Type": "",
        "ReleaseDate": "2024.06",
        "affiliation": "Alibaba",
        "Link": "https://huggingface.co/Qwen/Qwen2-0.5B",
        "AttentionType": "GQA",
        "LayerNumber": "24",
        "HiddenSize": "896",
        "HeadNum": "14",
        "Activation": "silu",
        "VocabularySize": "151936",
        "FFNratio": "5.428571428571429",
        "ArchitectureInnovation": "Dual Chunk Attention with YARN",
        "TrainingTokens": "12000B",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "32000",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.49266143830498926",
        "Problemsolving": "0.44246590020885446",
        "Math": "11.5",
        "IncontextLearning": ""
    },
    {
        "Model": "Qwen/Qwen2-1.5B",
        "Parameters": "1.5B",
        "Type": "",
        "ReleaseDate": "2024.06",
        "affiliation": "Alibaba",
        "Link": "https://huggingface.co/Qwen/Qwen2-1.5B",
        "AttentionType": "GQA",
        "LayerNumber": "28",
        "HiddenSize": "1536",
        "HeadNum": "12",
        "Activation": "silu",
        "VocabularySize": "151936",
        "FFNratio": "5.833333333333333",
        "ArchitectureInnovation": "Dual Chunk Attention with YARN",
        "TrainingTokens": "7000B",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "32000",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5862346076029146",
        "Problemsolving": "0.5678012985816538",
        "Math": "34.96666666666667",
        "IncontextLearning": ""
    },
    {
        "Model": "HuggingFaceTB/SmolLM-360M",
        "Parameters": "360M",
        "Type": "",
        "ReleaseDate": "2024.07",
        "affiliation": "HuggingFace",
        "Link": "https://huggingface.co/HuggingFaceTB/SmolLM-360M",
        "AttentionType": "GQA",
        "LayerNumber": "32",
        "HiddenSize": "960",
        "HeadNum": "15",
        "Activation": "silu",
        "VocabularySize": "49152",
        "FFNratio": "2.6666666666666665",
        "ArchitectureInnovation": "",
        "TrainingTokens": "600B",
        "TrainingDatasets": "FineWeb-Edu,Python-Edu,Cosmopedia v2",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.450186107",
        "Problemsolving": "0.44623236777646696",
        "Math": "2.6666666666666665",
        "IncontextLearning": ""
    },
    {
        "Model": "HuggingFaceTB/SmolLM-135M",
        "Parameters": "135M",
        "Type": "",
        "ReleaseDate": "2024.07",
        "affiliation": "HuggingFace",
        "Link": "https://huggingface.co/HuggingFaceTB/SmolLM-135M",
        "AttentionType": "GQA",
        "LayerNumber": "30",
        "HiddenSize": "576",
        "HeadNum": "9",
        "Activation": "silu",
        "VocabularySize": "49152",
        "FFNratio": "2.6666666666666665",
        "ArchitectureInnovation": "",
        "TrainingTokens": "600B",
        "TrainingDatasets": "FineWeb-Edu,Python-Edu,Cosmopedia v3",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4298944496121605",
        "Problemsolving": "0.40125269297749017",
        "Math": "2.266666666666667",
        "IncontextLearning": ""
    },
    {
        "Model": "HuggingFaceTB/SmolLM-1.7B",
        "Parameters": "1.7B",
        "Type": "",
        "ReleaseDate": "2024.07",
        "affiliation": "HuggingFace",
        "Link": "https://huggingface.co/HuggingFaceTB/SmolLM-1.7B",
        "AttentionType": "MHA",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "49152",
        "FFNratio": "4",
        "ArchitectureInnovation": "",
        "TrainingTokens": "1000B",
        "TrainingDatasets": "FineWeb-Edu,Python-Edu,Cosmopedia v4",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5014415770944014",
        "Problemsolving": "0.5041593497962163",
        "Math": "5.466666666666666",
        "IncontextLearning": ""
    },
    {
        "Model": "google/gemma-2-2b",
        "Parameters": "2B",
        "Type": "",
        "ReleaseDate": "2024.07",
        "affiliation": "Google ",
        "Link": "https://huggingface.co/google/gemma-2-2b",
        "AttentionType": "GQA",
        "LayerNumber": "26",
        "HiddenSize": "2304",
        "HeadNum": "8",
        "Activation": "gelu_pytorch_tanh",
        "VocabularySize": "256000",
        "FFNratio": "8",
        "ArchitectureInnovation": "Local Sliding Window and Global Attention, logits softcap",
        "TrainingTokens": "2000B",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "8192",
        "TrainingInnovations": "Distillation",
        "Commonsensereasoning": "0.5995555709284136",
        "Problemsolving": "0.6155073419975245",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "h2oai/h2o-danube3-4b-base",
        "Parameters": "4B",
        "Type": "",
        "ReleaseDate": "2024.07",
        "affiliation": "H2O",
        "Link": "https://huggingface.co/h2oai/h2o-danube3-4b-base",
        "AttentionType": "GQA",
        "LayerNumber": "24",
        "HiddenSize": "3840",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "6T",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.632912919",
        "Problemsolving": "0.5446465941035985",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "h2oai/h2o-danube3-500m-base",
        "Parameters": "0.5B",
        "Type": "",
        "ReleaseDate": "2024.07",
        "affiliation": "H2O",
        "Link": "https://huggingface.co/h2oai/h2o-danube3-4b-base",
        "AttentionType": "GQA",
        "LayerNumber": "16",
        "HiddenSize": "1536",
        "HeadNum": "16",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "4T",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.45932054811796474",
        "Problemsolving": "0.3050223415565104",
        "Math": "11.95",
        "IncontextLearning": ""
    },
    {
        "Model": "tensoropera/Fox-1-1.6B",
        "Parameters": "1.6B",
        "Type": "",
        "ReleaseDate": "2024.07",
        "affiliation": "TensorOpera AI",
        "Link": "https://huggingface.co/tensoropera/Fox-1-1.6B",
        "AttentionType": "GQA",
        "LayerNumber": "32",
        "HiddenSize": "2048",
        "HeadNum": "16",
        "Activation": "silu",
        "VocabularySize": "32000",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "3T",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "8192",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5646421735096055",
        "Problemsolving": "0.5014686489970536",
        "Math": "3.3000000000000003",
        "IncontextLearning": ""
    },
    {
        "Model": "TRI-ML/DCLM-1B",
        "Parameters": "1.4B",
        "Type": "",
        "ReleaseDate": "2024.08",
        "affiliation": "Toyota",
        "Link": "https://huggingface.co/TRI-ML/DCLM-1B",
        "AttentionType": "",
        "LayerNumber": "24",
        "HiddenSize": "2048",
        "HeadNum": "16",
        "Activation": "silu",
        "VocabularySize": "50432",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "4300B",
        "TrainingDatasets": "DCLM",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5874787522273434",
        "Problemsolving": "0.5659609420006273",
        "Math": "None",
        "IncontextLearning": ""
    },
    {
        "Model": "LiteAI/Hare-1.1B-base-0.5v",
        "Parameters": "1.1B",
        "Type": "",
        "ReleaseDate": "2024.09",
        "affiliation": "China Telecom",
        "Link": "https://huggingface.co/LiteAI/Hare-1.1B-base-0.5v",
        "AttentionType": "GQA",
        "LayerNumber": "22",
        "HiddenSize": "2048",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "48593",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "1.4T",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5244433299057257",
        "Problemsolving": "0.47808033252354437",
        "Math": "13.100000000000001",
        "IncontextLearning": ""
    },
    {
        "Model": "databricks/dolly-v2-3b",
        "Parameters": "3B",
        "Type": "Instruct",
        "ReleaseDate": "2023.04",
        "affiliation": "DataBricks",
        "Link": "https://huggingface.co/databricks/dolly-v2-3b",
        "AttentionType": "MHA",
        "LayerNumber": "32",
        "HiddenSize": "2560",
        "HeadNum": "32",
        "Activation": "gelu",
        "VocabularySize": "50280",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "closed dataset",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4509146197910751",
        "Problemsolving": "0.29783698635930334",
        "Math": "2.5",
        "IncontextLearning": ""
    },
    {
        "Model": "allenai/OLMo-1B-hf",
        "Parameters": "1.18B",
        "Type": "",
        "ReleaseDate": "2024.04",
        "affiliation": "AllenAI",
        "Link": "https://huggingface.co/allenai/OLMo-1B-hf",
        "AttentionType": "",
        "LayerNumber": "16",
        "HiddenSize": "2048",
        "HeadNum": "16",
        "Activation": "silu",
        "VocabularySize": "50304",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "3T",
        "TrainingDatasets": "Dolma ",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.451094964",
        "Problemsolving": "0.2910177317875724",
        "Math": "2.5",
        "IncontextLearning": ""
    },
    {
        "Model": "openai-community/gpt2",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "2019.02",
        "affiliation": "OpenAI",
        "Link": "https://huggingface.co/openai-community/gpt2",
        "AttentionType": "",
        "LayerNumber": "12",
        "HiddenSize": "768",
        "HeadNum": "12",
        "Activation": "gelu_new",
        "VocabularySize": "50257",
        "FFNratio": "4",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "1024",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.38620685273839245",
        "Problemsolving": "0.2990901618037843",
        "Math": "1.8",
        "IncontextLearning": ""
    },
    {
        "Model": "openai-community/gpt2-large",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "2019.02",
        "affiliation": "OpenAI",
        "Link": "https://huggingface.co/openai-community/gpt2-large",
        "AttentionType": "",
        "LayerNumber": "36",
        "HiddenSize": "1280",
        "HeadNum": "20",
        "Activation": "gelu_new",
        "VocabularySize": "50257",
        "FFNratio": "4",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "1024",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4358806599136828",
        "Problemsolving": "0.3500853101425672",
        "Math": "2.6",
        "IncontextLearning": ""
    },
    {
        "Model": "openai-community/gpt2-xl",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "2019.02",
        "affiliation": "OpenAI",
        "Link": "https://huggingface.co/openai-community/gpt2-xl",
        "AttentionType": "",
        "LayerNumber": "48",
        "HiddenSize": "1600",
        "HeadNum": "25",
        "Activation": "gelu_new",
        "VocabularySize": "50257",
        "FFNratio": "4",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "1024",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.44893756283325165",
        "Problemsolving": "0.3785298684041023",
        "Math": "2.8",
        "IncontextLearning": ""
    },
    {
        "Model": "openai-community/gpt2-medium",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "2019.02",
        "affiliation": "OpenAI",
        "Link": "https://huggingface.co/openai-community/gpt2-medium",
        "AttentionType": "",
        "LayerNumber": "24",
        "HiddenSize": "1024",
        "HeadNum": "16",
        "Activation": "gelu_new",
        "VocabularySize": "50257",
        "FFNratio": "4",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "1024",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.4205963548066571",
        "Problemsolving": "0.33778448420670787",
        "Math": "2",
        "IncontextLearning": ""
    },
    {
        "Model": "openbmb/MiniCPM3-4B",
        "Parameters": "4B",
        "Type": "",
        "ReleaseDate": "2024.09",
        "affiliation": "OpenBMB",
        "Link": "https://huggingface.co/openbmb/MiniCPM3-4B",
        "AttentionType": "MLA",
        "LayerNumber": "62",
        "HiddenSize": "2560",
        "HeadNum": "40",
        "Activation": "silu",
        "VocabularySize": "73448",
        "FFNratio": "2.5",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "32768",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.6453785870346623",
        "Problemsolving": "0.688412583",
        "Math": "",
        "IncontextLearning": ""
    },
    {
        "Model": "microsoft/Phi-3.5-mini-instruct",
        "Parameters": "",
        "Type": "Instruct",
        "ReleaseDate": "2024.09",
        "affiliation": "Microsoft",
        "Link": "https://huggingface.co/microsoft/Phi-3.5-mini-instruct",
        "AttentionType": "MHA",
        "LayerNumber": "32",
        "HiddenSize": "3072",
        "HeadNum": "32",
        "Activation": "silu",
        "VocabularySize": "32064",
        "FFNratio": "2.6666666666666665",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "262144",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.6770832948732616",
        "Problemsolving": "0.7158442652977252",
        "Math": "60.45",
        "IncontextLearning": ""
    },
    {
        "Model": "Qwen/Qwen2.5-1.5B",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "2024.09",
        "affiliation": "Alibaba",
        "Link": "https://huggingface.co/Qwen/Qwen2.5-1.5B",
        "AttentionType": "",
        "LayerNumber": "28",
        "HiddenSize": "1536",
        "HeadNum": "12",
        "Activation": "silu",
        "VocabularySize": "151936",
        "FFNratio": "5.833333333333333",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.6008633588934338",
        "Problemsolving": "0.6366175184377857",
        "Math": "44.75",
        "IncontextLearning": ""
    },
    {
        "Model": "Qwen/Qwen2.5-0.5B",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "2024.09",
        "affiliation": "Alibaba",
        "Link": "https://huggingface.co/Qwen/Qwen2.5-0.5B",
        "AttentionType": "",
        "LayerNumber": "24",
        "HiddenSize": "896",
        "HeadNum": "14",
        "Activation": "silu",
        "VocabularySize": "151936",
        "FFNratio": "5.428571428571429",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5075411932713384",
        "Problemsolving": "0.49727140659009444",
        "Math": "12.25",
        "IncontextLearning": ""
    },
    {
        "Model": "Qwen/Qwen2.5-3B",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "2024.09",
        "affiliation": "Alibaba",
        "Link": "https://huggingface.co/Qwen/Qwen2.5-3B",
        "AttentionType": "",
        "LayerNumber": "36",
        "HiddenSize": "2048",
        "HeadNum": "16",
        "Activation": "silu",
        "VocabularySize": "151936",
        "FFNratio": "5.375",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.6349671034914612",
        "Problemsolving": "0.6736793605222733",
        "Math": "58.55",
        "IncontextLearning": ""
    },
    {
        "Model": "Llama-7B",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "",
        "affiliation": "",
        "Link": "luodian/llama-7b-hf",
        "AttentionType": "",
        "LayerNumber": "",
        "HiddenSize": "",
        "HeadNum": "",
        "Activation": "",
        "VocabularySize": "",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "2048",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.5731176151179714",
        "Problemsolving": "0.5325788551139518",
        "Math": "6.35",
        "IncontextLearning": ""
    },
    {
        "Model": "Llama-2-7B",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "",
        "affiliation": "",
        "Link": "NousResearch/Llama-2-7b-hf",
        "AttentionType": "",
        "LayerNumber": "",
        "HiddenSize": "",
        "HeadNum": "",
        "Activation": "",
        "VocabularySize": "",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.6159583283691168",
        "Problemsolving": "0.5865883450406616",
        "Math": "9",
        "IncontextLearning": ""
    },
    {
        "Model": "Llama-3-7B",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "",
        "affiliation": "",
        "Link": "meta-llama/Meta-Llama-3-8B",
        "AttentionType": "",
        "LayerNumber": "",
        "HiddenSize": "",
        "HeadNum": "",
        "Activation": "",
        "VocabularySize": "",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.6624167482775103",
        "Problemsolving": "0.6835743047201107",
        "Math": "34.65",
        "IncontextLearning": ""
    },
    {
        "Model": "Llama-3.1-7B",
        "Parameters": "",
        "Type": "",
        "ReleaseDate": "",
        "affiliation": "",
        "Link": "meta-llama/Meta-Llama-3.1-8B",
        "AttentionType": "",
        "LayerNumber": "",
        "HiddenSize": "",
        "HeadNum": "",
        "Activation": "",
        "VocabularySize": "",
        "FFNratio": "",
        "ArchitectureInnovation": "",
        "TrainingTokens": "",
        "TrainingDatasets": "",
        "GPUhours": "",
        "MaxContextWindow": "",
        "TrainingInnovations": "",
        "Commonsensereasoning": "0.668221052",
        "Problemsolving": "0.6834280443737909",
        "Math": "37.15",
        "IncontextLearning": ""
    }
]